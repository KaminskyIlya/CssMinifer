Попробовать парсить стили вначале просто как текст.
Применить технику YUICompressor - замена "проблемных" частей на временные идентификаторы через регулярные выражения.
"Проблемными" могут быть:
  комментарии блочные  /* */  <!-- -->
  комментарии строчные //
  строки, ссылки
  dataURI
  функции  (calc, linear-gradient, filter, attr, rgba, hsl, ...)
  выражения ( '(' attr(href) ')' )
  селекторы (a + p > h1:active ~ em)
  медиа-запросы (@media screen and (max-width: 1024px))
  условия ( @supports ( display: flexbox ) or ( box-shadow: 2px 2px 2px black ) )

Тогда после обработки таблица стилей может выглядеть так:
a,b,c
{
  background-image:!A;
  content:!B;
  filter:!F;
  display: none;
  padding: 0.5em;
}
@a expr { ... }
Причем если селектору "a" соответствует, например p > em, то везде, где встретится этот же селектор, он будет заменен на "a".
Это поможет искать повторение селекторов и делать их перегруппировку.

Во время такой обработки можно не проверять дотошно синтаксическую правильность текста. Это можно сделать уже потом,
при внимательном анализе выражений, строк, ссылок, селекторов. Зато на данном этапе можно запомнить где в исходном
тексте находился тот или иной элемент*. Единственное - придется проверять, что каждому { соответствует закрытый }.
Если это не так - мы найдем { раньше, чем } или встретим конец файла. Но на данном этапе мы можем запомнить,
где начинается и заканчивается каждый блок {} для каждой группы селекторов*. Это нам может упростить работу.
По итогу мы должны получить такой псевдо-файл:
selectors_list{declarations_list}
@media_query{selectors_list{declarations_list}}
@font-face{declarations_list}
@keyframes{index{declarations_list}index{declarations_list}}

*фактически мы тут пришли к технике токенизации - замена сложных конструкций более простыми токенами.
Набор токенов можно сократить. Токен может хранить информацию о своем местоположении в исходном тексте,
знать свою длину, а также, возможно, проверять свою синтаксическую правильность.
На этапе токенизации мы можем обработать "несмертельные" ошибки.
Например, при токенизации строки, мы можем встретить символы ; \r } раньше, чем символ закрытия строки.
Тогда можно сообщить об ошибке. Показать, как она будет исправлена.

Удобство токенизации еще и в том, что синтаксическую правильность будет анализировать один класс. И логика проверки
не будет разбросана по куче разных классов, между которыми необходимо будет передавать информацию (спагетти-код).

Затем мы можем углубить токенизацию, например так:
selectors_list = selector,selector,selector
declarations_list = property:value;property:value



Теперь про интеллектуальную оптимизацию.
Вначале попробуем сгруппировать селекторы, у которых повторяются свойства (выносим повторения за скобки - в новые стили).
Затем уже делаем оптимизации внутри блоков. Тут есть сложности. Есть эквивалентные свойства, которые не должны быть перезаписаны:
color: red;
color: rgba(255, 0, 0, 0.5);

Числа и цвета будем оптимизировать на данном этапе, работая с моделью.
Оптимизация функций дело вообще сложное. Один только linear-gradient чего стоит.
